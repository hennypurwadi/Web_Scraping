{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa75da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup \n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import schedule\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd06dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create directory\n",
    "import os\n",
    "\n",
    "path = './data'\n",
    "# Check whether the specified path exists or not\n",
    "isExist = os.path.exists(path)\n",
    "\n",
    "if not isExist: \n",
    "  # Create a new directory because it does not exist \n",
    "  os.makedirs(path)\n",
    "  print(\"new directory is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cefe8864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-02-20'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = datetime.now().date()\n",
    "time_delta_D = timedelta(days=7)\n",
    "lastweek = today - time_delta_D\n",
    "\n",
    "today = str(today)\n",
    "lastweek = str(lastweek)\n",
    "\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec4de98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://finance.yahoo.com/cryptocurrencies/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://finance.yahoo.com/gainers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://finance.yahoo.com/currencies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://coinmarketcap.com/nft/collections/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      url_list\n",
       "0  https://finance.yahoo.com/cryptocurrencies/\n",
       "1            https://finance.yahoo.com/gainers\n",
       "2         https://finance.yahoo.com/currencies\n",
       "3   https://coinmarketcap.com/nft/collections/"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_url = pd.read_csv('url.csv')\n",
    "file_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc2161f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://finance.yahoo.com/cryptocurrencies/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_url['url_list'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5509b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scrape(url):\n",
    "    \n",
    "    Bsoup = BeautifulSoup(requests.get(url).text)\n",
    "    headers = [header.text for listing in Bsoup.find_all('thead') for header in listing.find_all('th')]\n",
    "    r_data = {header:[] for header in headers}\n",
    "\n",
    "    for rows in Bsoup.find_all('tbody'):\n",
    "      for row in rows.find_all('tr'):\n",
    "        \n",
    "        if len(row) != len(headers): continue\n",
    "        for idx, cell in enumerate(row.find_all('td')):\n",
    "              r_data[headers[idx]].append(cell.text)\n",
    "    return pd.DataFrame(r_data)\n",
    "\n",
    "\n",
    "def job():\n",
    "\n",
    "    df1 = web_scrape(file_url['url_list'].iloc[0])\n",
    "    df = df1.iloc[:, 0:3]\n",
    "    \n",
    "    h5File = (today + '_d_web_scrape.h5')\n",
    "    df.to_hdf(h5File, 'w')\n",
    "    print(\"wrote hdf5 file done\")   \n",
    "    \n",
    "schedule.every(1).minutes.do(job)\n",
    "schedule.every(1).hour.do(job)\n",
    "schedule.every(1).day.at(\"10:30\").do(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671abe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = web_scrape(file_url['url_list'].iloc[0])\n",
    "df = df1.iloc[:, 0:3]\n",
    "\n",
    "h5File = (today + '_d_web_scrape.h5')\n",
    "df.to_hdf(h5File, 'w')\n",
    "print(\"wrote hdf5 file done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = pd.read_hdf(h5File)\n",
    "hf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(hf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65836ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile web_scraping.py\n",
    "\n",
    "import requests \n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup \n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import schedule\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "today = datetime.now().date()\n",
    "today = str(today)\n",
    "\n",
    "file_url = pd.read_csv('url.csv')\n",
    "\n",
    "def web_scrape(url):\n",
    "    \n",
    "    Bsoup = BeautifulSoup(requests.get(url).text)\n",
    "    headers = [header.text for listing in Bsoup.find_all('thead') for header in listing.find_all('th')]\n",
    "    r_data = {header:[] for header in headers}\n",
    "\n",
    "    for rows in Bsoup.find_all('tbody'):\n",
    "      for row in rows.find_all('tr'):\n",
    "        \n",
    "        if len(row) != len(headers): continue\n",
    "        for idx, cell in enumerate(row.find_all('td')):\n",
    "              r_data[headers[idx]].append(cell.text)\n",
    "    return pd.DataFrame(r_data)\n",
    "\n",
    "\n",
    "def job():\n",
    "\n",
    "    df1 = web_scrape(file_url['url_list'].iloc[0])\n",
    "    df = df1.iloc[:, 0:3]\n",
    "    \n",
    "    h5File = (today + '_d_web_scrape.h5')\n",
    "    df.to_hdf(h5File, 'w')\n",
    "    print(\"wrote hdf5 file done\")   \n",
    "    \n",
    "#schedule.every().day.at(\"10:30\").do(job)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    job()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
